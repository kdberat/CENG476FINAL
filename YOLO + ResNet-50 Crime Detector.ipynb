{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO+ResNet-50 based Crime Detection System:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T09:57:32.191435Z",
     "start_time": "2021-05-13T09:57:31.994448Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# PART-1: Define all the functions which will combine both yolo and crime detection model\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def dateTimeNow():\n",
    "    day=str(time.localtime().tm_mday)\n",
    "    mon=str(time.localtime().tm_mon)\n",
    "    yer=str(time.localtime().tm_year)\n",
    "    tim=str(time.localtime().tm_hour)+\"-\"+str(time.localtime().tm_min)+\"-\"+str(time.localtime().tm_sec)\n",
    "    return day+\"-\"+mon+\"-\"+yer+\"_\"+tim\n",
    "\n",
    "\n",
    "def setState(pred):\n",
    "    ret = None\n",
    "    if pred>0.4:\n",
    "        ret = \"Crime\"\n",
    "    else:\n",
    "        ret = \"Normal\"\n",
    "        \n",
    "    return f\"{ret} ({round(pred*100, 3)}%)\"\n",
    "\n",
    "\n",
    "def makePrediction( model, color=\"green\", buffer_size=90, yolo_version=4,  min_conf=0.1):\n",
    "    \n",
    "    VERSION = yolo_version\n",
    "    snaps = 0\n",
    "    \n",
    "    colors = {\n",
    "        \"black\": (0, 0, 0),\n",
    "        \"white\": (255, 255, 255),\n",
    "        \"red\": (0, 0, 255),\n",
    "        \"green\": (0, 255, 0),\n",
    "        \"blue\": (255, 0, 0),\n",
    "    }\n",
    "    \n",
    "    selected_color=colors[color]\n",
    "\n",
    "    net = cv2.dnn.readNet(\n",
    "        f\"C:/Users/berat/Desktop/CENG476FINAL/coco/yolov3-tiny.weights\",\n",
    "        f\"C:/Users/berat/Desktop/CENG476FINAL/coco/yolov3-tiny.cfg\",\n",
    "    )\n",
    "   \n",
    "\n",
    "    classes = []\n",
    "    with open(\"C:/Users/berat/Desktop/CENG476FINAL/coco/coco.names\",\"r\") as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    buffer = []\n",
    "    prev_pred = \"NaN\"\n",
    "\n",
    "    layer_names = net.getLayerNames()\n",
    "    outputlayers = net.getUnconnectedOutLayersNames()  \n",
    "\n",
    "\n",
    "    colors= np.random.uniform(0,255,size=(len(classes),3))\n",
    "\n",
    "    cap=cv2.VideoCapture(f\"C:/Users/berat/Desktop/CENG476FINAL/videos/crime/burg.mp4\")\n",
    "\n",
    "    while True:\n",
    "        state = f\"Human Detection: False; State: {prev_pred}\"\n",
    "        ret,frame= cap.read() \n",
    "\n",
    "        if ret==True:\n",
    "            height,width,channels = frame.shape\n",
    "            blob = cv2.dnn.blobFromImage(frame,0.00392,(320,320),(0,0,0),True,crop=False)    \n",
    "\n",
    "            net.setInput(blob)\n",
    "            outs = net.forward(outputlayers)\n",
    "\n",
    "            class_ids=[]\n",
    "            confidences=[]\n",
    "            boxes=[]\n",
    "\n",
    "            for out in outs:\n",
    "                for detection in out:\n",
    "                    scores = detection[5:]\n",
    "                    class_id = np.argmax(scores)\n",
    "                    confidence = scores[class_id]\n",
    "\n",
    "                    # could modify this to meet your requirements\n",
    "                    if confidence > min_conf:\n",
    "                        #object detected\n",
    "                        center_x= int(detection[0]*width)\n",
    "                        center_y= int(detection[1]*height)\n",
    "                        w = int(detection[2]*width)\n",
    "                        h = int(detection[3]*height)\n",
    "\n",
    "                        #rectangle co-ordinaters\n",
    "                        x=int(center_x - w/2)\n",
    "                        y=int(center_y - h/2)\n",
    "\n",
    "                        boxes.append([x,y,w,h])\n",
    "                        confidences.append(float(confidence))\n",
    "                        class_ids.append(class_id)\n",
    "\n",
    "            indexes = cv2.dnn.NMSBoxes(boxes,confidences,0.4,0.6)\n",
    "\n",
    "            for i in range(len(boxes)):\n",
    "                if i in indexes:\n",
    "                    label = str(classes[class_ids[i]])\n",
    "                    confidence= confidences[i]\n",
    "\n",
    "                    if label==\"person\":\n",
    "                        x,y,w,h = boxes[i]\n",
    "\n",
    "                        # Custom code to do miscellaneous tasks (like predicting anomalies) can go down here\n",
    "                        out = frame.copy()\n",
    "                        out = cv2.cvtColor(out, cv2.COLOR_BGR2RGB)\n",
    "                        out = cv2.resize(out, (320, 240)).astype(\"float32\")\n",
    "                        \n",
    "                        pred = model.predict(np.expand_dims(out, axis=0))[0][0]\n",
    "                        \n",
    "                        # Update buffer for smoothing out predictions over time\n",
    "                        if len(buffer)<buffer_size:\n",
    "                            buffer.append(pred)\n",
    "                            temp = setState(pred)\n",
    "                        else:\n",
    "                            buffer.pop(0)\n",
    "                            buffer.append(pred)\n",
    "                            temp = setState(sum(buffer)/buffer_size)\n",
    "                        prev_pred = temp\n",
    "                        \n",
    "                        state = f\"Human Detection: True; State: {prev_pred}\"\n",
    "                        # Custom code ends\n",
    "\n",
    "                    color = colors[class_ids[i]]\n",
    "                    cv2.rectangle(frame,(x,y),(x+w,y+h),color,2)\n",
    "                    cv2.putText(frame,label+\":\"+str(round(confidence*100,2))+\"%\", (x, y-10), cv2.FONT_HERSHEY_SIMPLEX,0.4,(255,255,255),2)\n",
    "\n",
    "            cv2.putText(frame, state, (0,20), cv2.FONT_HERSHEY_SIMPLEX, 0.4, selected_color, 2)\n",
    "\n",
    "            cv2.imshow(\"Image\",frame)\n",
    "            key = cv2.waitKey(1)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF==ord('q'):\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "\n",
    "# PART-2: Import the video files\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T09:58:14.457780Z",
     "start_time": "2021-05-13T09:58:01.742751Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# PART-3: Import tensorflow and load model\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.load_model(f\"C:/Users/berat/Desktop/CENG476FINAL/model/model_x-y-21.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T11:05:34.306319Z",
     "start_time": "2021-05-13T11:05:19.818578Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 232ms/step\n",
      "1/1 [==============================] - 0s 226ms/step\n",
      "1/1 [==============================] - 0s 219ms/step\n",
      "1/1 [==============================] - 0s 215ms/step\n",
      "1/1 [==============================] - 0s 213ms/step\n",
      "1/1 [==============================] - 0s 201ms/step\n",
      "1/1 [==============================] - 0s 205ms/step\n",
      "1/1 [==============================] - 0s 209ms/step\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 197ms/step\n",
      "1/1 [==============================] - 0s 184ms/step\n",
      "1/1 [==============================] - 0s 207ms/step\n",
      "1/1 [==============================] - 0s 204ms/step\n",
      "1/1 [==============================] - 0s 224ms/step\n",
      "1/1 [==============================] - 0s 203ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 205ms/step\n",
      "1/1 [==============================] - 0s 201ms/step\n",
      "1/1 [==============================] - 0s 195ms/step\n",
      "1/1 [==============================] - 0s 207ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 206ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 187ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 224ms/step\n",
      "1/1 [==============================] - 0s 204ms/step\n",
      "1/1 [==============================] - 0s 225ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 193ms/step\n",
      "1/1 [==============================] - 0s 199ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 187ms/step\n",
      "1/1 [==============================] - 0s 187ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "1/1 [==============================] - 0s 193ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 184ms/step\n",
      "1/1 [==============================] - 0s 192ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 187ms/step\n",
      "1/1 [==============================] - 0s 184ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 178ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 192ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 209ms/step\n",
      "1/1 [==============================] - 0s 211ms/step\n",
      "1/1 [==============================] - 0s 197ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 286ms/step\n",
      "1/1 [==============================] - 0s 233ms/step\n",
      "1/1 [==============================] - 0s 383ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m folder \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mC:/Users/berat/Desktop/CENG476FINAL//videos/crime\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m video_number\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m----> 4\u001b[0m makePrediction(\n\u001b[0;32m      5\u001b[0m    \n\u001b[0;32m      6\u001b[0m     model, \n\u001b[0;32m      7\u001b[0m     color\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mred\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[0;32m      8\u001b[0m     buffer_size\u001b[39m=\u001b[39;49m\u001b[39m90\u001b[39;49m, \n\u001b[0;32m      9\u001b[0m     yolo_version\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, \n\u001b[0;32m     10\u001b[0m     min_conf\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m\n\u001b[0;32m     11\u001b[0m )\n",
      "Cell \u001b[1;32mIn[1], line 69\u001b[0m, in \u001b[0;36mmakePrediction\u001b[1;34m(model, color, buffer_size, yolo_version, min_conf)\u001b[0m\n\u001b[0;32m     66\u001b[0m blob \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mdnn\u001b[39m.\u001b[39mblobFromImage(frame,\u001b[39m0.00392\u001b[39m,(\u001b[39m320\u001b[39m,\u001b[39m320\u001b[39m),(\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m),\u001b[39mTrue\u001b[39;00m,crop\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)    \n\u001b[0;32m     68\u001b[0m net\u001b[39m.\u001b[39msetInput(blob)\n\u001b[1;32m---> 69\u001b[0m outs \u001b[39m=\u001b[39m net\u001b[39m.\u001b[39;49mforward(outputlayers)\n\u001b[0;32m     71\u001b[0m class_ids\u001b[39m=\u001b[39m[]\n\u001b[0;32m     72\u001b[0m confidences\u001b[39m=\u001b[39m[]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "folder = \"C:/Users/berat/Desktop/CENG476FINAL//videos/crime\"\n",
    "video_number=1\n",
    "\n",
    "makePrediction(\n",
    "   \n",
    "    model, \n",
    "    color=\"red\", \n",
    "    buffer_size=90, \n",
    "    yolo_version=4, \n",
    "    min_conf=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
